(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{246:function(e,n,t){"use strict";t.r(n);var o=t(12),r=t(14),l=function(e,n,t,desc){var o,r=arguments.length,l=r<3?n:null===desc?desc=Object.getOwnPropertyDescriptor(n,t):desc;if("object"==typeof Reflect&&"function"==typeof Reflect.decorate)l=Reflect.decorate(e,n,t,desc);else for(var i=e.length-1;i>=0;i--)(o=e[i])&&(l=(r<3?o(l):r>3?o(n,t,l):o(n,t))||l);return r>3&&l&&Object.defineProperty(n,t,l),l},c=function(e){function n(){return e.apply(this,arguments)||this}return Object(o.a)(n,e),n}(r.Vue),d=c=l([Object(r.Component)({})],c),h=t(20),component=Object(h.a)(d,(function(){var e=this.$createElement;return(this._self._c||e)("div",[this._v("\n  # PyTorch Mobile\n\n  Running ML on edge devices is growing in importance as applications continue to demand lower latency. It is also a foundational element for privacy-preserving techniques such as federated learning. As of PyTorch 1.3, PyTorch supports an end-to-end workflow from Python to deployment on iOS and Android.\n\n  This is an early, experimental release that we will be building on in several areas over the coming months:\n\n  - Provide APIs that cover common preprocessing and integration tasks needed for incorporating ML in mobile applications\n  - Support for QNNPACK quantized kernel libraries and support for ARM CPUs\n  - Build level optimization and selective compilation depending on the operators needed for user applications (i.e., you pay binary size for only the operators you need)\n  - Further improvements to performance and coverage on mobile CPUs and GPUs\n\n  Learn more or get started on [Android]("+this._s(this.site.baseurl)+"/mobile/android) or [iOS]("+this._s(this.site.baseurl)+"/mobile/ios).\n\n  "),this._m(0)])}),[function(){var e=this.$createElement,n=this._self._c||e;return n("div",{staticClass:"text-center"},[n("img",{attrs:{src:"/images/pytorch-mobile.png",width:"100%"}})])}],!1,null,null,null);n.default=component.exports}}]);